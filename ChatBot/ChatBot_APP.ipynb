{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d835242",
   "metadata": {},
   "source": [
    "# ChatBot APP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409ff2b9",
   "metadata": {},
   "source": [
    "**Note**: In June 2023, OpenAI updated gpt-3.5-turbo. The results you see in the notebook may be slightly different than those in the video. Some of the prompts have also been slightly modified to product the desired results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ddf3b-d921-4169-9d82-b0ee95e9eccd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's import necessary libraries: os, openai, and dotenv.\n",
    "\n",
    "Then, we load environment variables from a .env file using load_dotenv and find_dotenv functions from the dotenv library.\n",
    "\n",
    "Finally, it sets the API key for the OpenAI library by retrieving it from the environment variables using `os.getenv('OPENAI_API_KEY')`. This API key allows access to OpenAI's services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2800e646",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417afcbf",
   "metadata": {},
   "source": [
    "We have two functions here. The first one, `get_completion`, takes a prompt and an optional model parameter, which defaults to \"gpt-3.5-turbo\". It creates a list of messages with a user role containing the provided prompt. Then, it uses OpenAI's class `ChatCompletion` to generate a response based on the given prompt. The `temperature` parameter controls the randomness of the model's output. Finally, it returns the content of the generated response.\n",
    "\n",
    "In the second function, `get_completion_from_messages`, instead of kind of putting a single prompt as input and getting a single completion, we're going to pass in a list of messages. And these messages can be kind of from a variety of different roles, so Let's describe those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6f2de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def get_completion_from_messages(messages, model=\"gpt-3.5-turbo\", temperature=0):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "#     print(str(response.choices[0].message))\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc2b32f-73da-4ef0-81f6-0c7bff70783d",
   "metadata": {},
   "source": [
    "Let's try to use these messages in a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8dfa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be90e47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
    "{'role':'user', 'content':'Hi, my name is Isa'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d0dab",
   "metadata": {},
   "source": [
    "**Note**      \n",
    "Each Conversation with a language model as a standalone interaction, which means that you must provide all relevant messages for the model to draw from in the current conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fa6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},    \n",
    "{'role':'user', 'content':'Yes,  can you remind me, What is my name?'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd6fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages =  [  \n",
    "{'role':'system', 'content':'You are friendly chatbot.'},\n",
    "{'role':'user', 'content':'Hi, my name is Isa'},\n",
    "{'role':'assistant', 'content': \"Hi Isa! It's nice to meet you. \\\n",
    "Is there anything I can help you with today?\"},\n",
    "{'role':'user', 'content':'Yes, you can remind me, What is my name?'}  ]\n",
    "response = get_completion_from_messages(messages, temperature=1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d2bbb7",
   "metadata": {},
   "source": [
    "## OrderBot\n",
    "Going to build your own chatbot.This chat bot is going to be called Autobot.And.We're going to automate the collection of user prompts and assistant responses in order to build this order box.And it's going to take orders at a pizza restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1506c5",
   "metadata": {},
   "source": [
    "So first we're going to define `collect_messages()` function. And what this is doing is it's going to kind of collect our user messages so we can avoid typing them in by hand in the way that we did above. And this is going to kind of collect prompts from a user interface that we'll build below.\n",
    "And then append it to a list called context and then it will call the model with that context every time, the royal response is then also added to the contact. So the kind of model message is added to the context, the user message is added to the context, so on. So it just kind of grows longer and longer.\n",
    "This way the model has the information it needs to determine what to do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f658d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = get_completion_from_messages(context) \n",
    "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response, width=600, style={'background-color': '#F6F6F6'})))\n",
    " \n",
    "    return pn.Column(*panels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bd16ad",
   "metadata": {},
   "source": [
    "And so here's the context, and it contains the system message that contains the menu.     \n",
    "**Note**    \n",
    "Every time we call the language model, we're going to use the same context, and the context is building up overtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a8a06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "\n",
    "panels = [] # collect display \n",
    "\n",
    "context = [ {'role':'system', 'content':\"\"\"\n",
    "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
    "You first greet the customer, then collects the order, \\\n",
    "and then asks if it's a pickup or delivery. \\\n",
    "You wait to collect the entire order, then summarize it and check for a final \\\n",
    "time if the customer wants to add anything else. \\\n",
    "If it's a delivery, you ask for an address. \\\n",
    "Finally you collect the payment.\\\n",
    "Make sure to clarify all options, extras and sizes to uniquely \\\n",
    "identify the item from the menu.\\\n",
    "You respond in a short, very conversational friendly style. \\\n",
    "The menu includes \\\n",
    "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
    "cheese pizza   10.95, 9.25, 6.50 \\\n",
    "eggplant pizza   11.95, 9.75, 6.75 \\\n",
    "fries 4.50, 3.50 \\\n",
    "greek salad 7.25 \\\n",
    "Toppings: \\\n",
    "extra cheese 2.00, \\\n",
    "mushrooms 1.50 \\\n",
    "sausage 3.00 \\\n",
    "canadian bacon 3.50 \\\n",
    "AI sauce 1.50 \\\n",
    "peppers 1.00 \\\n",
    "Drinks: \\\n",
    "coke 3.00, 2.00, 1.00 \\\n",
    "sprite 3.00, 2.00, 1.00 \\\n",
    "bottled water 5.00 \\\n",
    "\"\"\"} ]  # accumulate messages\n",
    "\n",
    "\n",
    "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text hereâ€¦')\n",
    "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
    "\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46593ec0",
   "metadata": {},
   "source": [
    "And so now we can ask the model to create a JSON summary that we could send to the order system based on the conversation.So we're now appending another system message which is an instruction and we're saying create a JSON summary of the previous food order itemize the price for each item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9455d58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages =  context.copy()\n",
    "messages.append(\n",
    "{'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\n",
    " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size  5)total price '},    \n",
    ")\n",
    " #The fields should be 1) pizza, price 2) list of toppings 3) list of drinks, include size include price  4) list of sides include size include price, 5)total price '},    \n",
    "\n",
    "response = get_completion_from_messages(messages, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a896b93",
   "metadata": {},
   "source": [
    "And notice in this case we're using lower temperature because for these kinds of tasks we want the output to be fairly predictable. For a conversational agent you might want to use a higher temperature. However in this case I would maybe use a lower temperature as well, because for our customers assistant shop what you might want the output to be a bit more predictable as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
